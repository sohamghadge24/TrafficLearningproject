# ğŸ“š PROJECT FILE STRUCTURE & PURPOSE GUIDE

## Project Overview

This is a **Traffic Signal Control using Multi-Agent Deep Reinforcement Learning (MADRL)** system. The project trains intelligent agents to optimize traffic light timing using PPO (Proximal Policy Optimization) with Lagrangian constraints for safety.

---

## ğŸ“ DIRECTORY STRUCTURE & FILE PURPOSES

### ğŸ¯ ROOT DIRECTORY (`/Users/sohamghadge/Documents/Final_Project`)

#### **EXECUTION & SETUP SCRIPTS**

| File | Purpose | What It Does |
|------|---------|------------|
| `run_agent.py` | **Main execution wrapper** | Activates venv and runs the training script with professional output formatting |
| `run_agent.sh` | **Bash execution script** | Alternative bash wrapper to run the agent |
| `reset_environment.sh` | **Environment reset** | Cleans up caches, terminates processes, resets logs for fresh runs |
| `verify_setup.py` | **System verification** | Checks if all dependencies, files, and configurations are in place |

#### **TESTING SCRIPTS**

| File | Purpose | What It Does |
|------|---------|------------|
| `test_quick.py` | **SUMO initialization test** | Verifies SUMO can start and connect to TraCI |
| `test_main_quick.py` | **Full pipeline test** | Runs 2 episodes to verify entire training loop works |
| `baseline_demo.py` | **Progress bar demo** | Shows what baseline evaluation output looks like |

#### **EXAMPLE & REFERENCE SCRIPTS**

| File | Purpose | What It Does |
|------|---------|------------|
| `sumo_example.py` | **SUMO usage example** | Demonstrates how to interact with SUMO simulator |
| `demo.html` | **Web demo** | HTML file for visualization (optional) |

#### **DOCUMENTATION FILES**

| File | Purpose |
|------|---------|
| `README.md` | Original project documentation |
| `README_AGENT.md` | Comprehensive agent usage guide |
| `QUICK_START.txt` | Quick reference for running the agent |
| `RUN_AGENT.md` | Detailed execution walkthrough |
| `AGENT_READY.md` | Setup instructions and getting started |
| `FINAL_STATUS.md` | Complete project status report |
| `CHANGES_SUMMARY.md` | Summary of all modifications |
| `BASELINE_ENHANCEMENT.md` | Details on percentage progress bars |
| `ENHANCEMENT_SUMMARY.md` | Feature summary and usage |
| `PERCENTAGE_BARS_QUICK_REF.md` | Quick reference for progress bars |
| `VALIDATION_REPORT.md` | Test results and system validation |
| `EXECUTION_SUMMARY.md` | Execution details and results |
| `ENHANCEMENT_DISPLAY.sh` | Display enhancement status |

---

### ğŸ—ï¸ TRAFFICLEARNINGPROJECT/SRC (Main Source Code)

#### **CORE TRAINING FILE**

| File | Purpose | Key Components |
|------|---------|-----------------|
| `main.py` | **Main training orchestration** | â€¢ Baseline evaluation (Fixed-Time, Actuated controllers)<br>â€¢ MADRL training loop (200 episodes)<br>â€¢ TensorBoard logging<br>â€¢ Before/after comparison display<br>â€¢ Progress bars for baseline tests |

#### **CONFIGURATION**

| File | Purpose | What It Does |
|------|---------|------------|
| `config.py` | **Central configuration hub** | â€¢ Defines all hyperparameters (learning rates, episodes, etc.)<br>â€¢ Sets up file paths<br>â€¢ Dynamic scenario/city selection<br>â€¢ Creates necessary directories |
| `__init__.py` | **Package initialization** | Imports submodules and configurations<br>Logs that package loaded successfully |

---

### ğŸ® ENV SUBDIRECTORY (Environment & SUMO Interaction)

#### **ENVIRONMENT WRAPPER**

| File | Purpose | What It Does |
|------|---------|------------|
| `traffic_env.py` | **RL Environment wrapper** | â€¢ Bridges RL training with SUMO simulator<br>â€¢ Provides `reset()` and `step()` methods<br>â€¢ Collects states from SUMO<br>â€¢ Returns rewards and costs to trainer<br>â€¢ Manages episode termination |

#### **SUMO INTERFACE**

| File | Purpose | What It Does |
|------|---------|------------|
| `sumo_interface.py` | **Low-level SUMO control** | â€¢ Starts/stops SUMO process<br>â€¢ Sends traffic light actions via TraCI<br>â€¢ Reads vehicle states from SUMO<br>â€¢ Calculates reward (speed-based)<br>â€¢ Calculates cost (stop ratio)<br>â€¢ Manages metrics collection |

#### **DEBUGGING**

| File | Purpose | What It Does |
|------|---------|------------|
| `debug_sumo.py` | **SUMO debugging utility** | Helpers for debugging SUMO issues<br>Output formatting for inspection |
| `__init__.py` | **Package initialization** | Imports environment modules |

---

### ğŸ¤– MADRL SUBDIRECTORY (Reinforcement Learning Algorithm)

#### **MAIN TRAINER**

| File | Purpose | What It Does |
|------|---------|------------|
| `ppo_trainer.py` | **PPO algorithm implementation** | â€¢ Collects experience from environment<br>â€¢ Stores in replay buffer<br>â€¢ Performs PPO updates (clipped objective)<br>â€¢ Manages Lagrange multiplier (for constraints)<br>â€¢ Enforces safety constraints (cost limits)<br>â€¢ Saves/loads models |

#### **REPLAY BUFFER**

| File | Purpose | What It Does |
|------|---------|------------|
| `buffer.py` | **Experience replay storage** | â€¢ Stores states, actions, rewards, costs<br>â€¢ Provides batch sampling for training<br>â€¢ Tracks episode metrics<br>â€¢ Computes advantages (GAE) |

#### **PACKAGE INIT**

| File | Purpose |
|------|---------|
| `__init__.py` | Imports trainer and buffer classes |

---

### ğŸ‘¥ AGENTS SUBDIRECTORY (Neural Networks)

#### **ACTOR NETWORK**

| File | Purpose | What It Does |
|------|---------|------------|
| `actor.py` | **Policy network** | â€¢ Multi-layer neural network<br>â€¢ Takes state as input<br>â€¢ Outputs action probabilities (2 actions)<br>â€¢ Used by trainer for action selection<br>â€¢ Updated via policy gradient |

#### **CRITIC NETWORK**

| File | Purpose | What It Does |
|------|---------|------------|
| `critic.py` | **Value network** | â€¢ Multi-layer neural network<br>â€¢ Takes state as input<br>â€¢ Outputs state value estimate<br>â€¢ Outputs cost value estimate<br>â€¢ Used to compute advantages<br>â€¢ Updated via MSE loss |

#### **PACKAGE INIT**

| File | Purpose |
|------|---------|
| `__init__.py` | Imports actor and critic networks |

---

### ğŸ“Š LOGS DIRECTORY (Training Outputs)

| File/Folder | Purpose | Contents |
|-------------|---------|----------|
| `metrics.json` | **All training metrics** | Episode-by-episode: reward, cost, scenario, steps<br>Also: best_reward, best_episode metadata |
| `baseline_results.csv` | **Baseline comparison** | Fixed-Time and Actuated controller results<br>Used for before/after comparison |
| `tensorboard/` | **TensorBoard event logs** | Event files for visualization<br>Contains: rewards, costs, losses, lagrange multiplier |

---

### ğŸ¯ MODELS DIRECTORY (Saved Neural Networks)

| File | Purpose | What It Stores |
|------|---------|----------------|
| `actor_final.pt` | **Final trained actor** | Policy network weights after 200 episodes |
| `critic_final.pt` | **Final trained critic** | Value network weights after 200 episodes |
| `actor_best.pt` | **Best episode actor** | Actor weights from highest reward episode |
| `critic_best.pt` | **Best episode critic** | Critic weights from highest reward episode |
| `actor_ep*.pt` | **Episode checkpoints** | Actor saved every 20 episodes (ep20, ep40, etc.) |
| `critic_ep*.pt` | **Episode checkpoints** | Critic saved every 20 episodes (ep20, ep40, etc.) |

---

### ğŸ“ SCENARIOS DIRECTORY (Traffic Network Configuration)

#### **ORIGINAL 4X4 GRID**

| File | Purpose |
|------|---------|
| `4x4_grid/osm.net.xml.gz` | Network file (compressed): defines 16 intersections, roads, connections |
| `4x4_grid/low_traffic.rou.xml` | Low traffic scenario: vehicle routes for sparse traffic |
| `4x4_grid/medium_traffic.rou.xml` | Medium traffic scenario: balanced traffic density |
| `4x4_grid/high_traffic.rou.xml` | High traffic scenario: dense traffic conditions |

#### **ORGANIZED CITY STRUCTURE**

| Path | Purpose |
|------|---------|
| `city4x4/low/osm.sumocfg` | SUMO config for 4x4 grid, low traffic |
| `city4x4/medium/osm.sumocfg` | SUMO config for 4x4 grid, medium traffic |
| `city4x4/high/osm.sumocfg` | SUMO config for 4x4 grid, high traffic |

---

## ğŸ”„ EXECUTION FLOW

### **When You Run `python3 run_agent.py`:**

```
1. run_agent.py
   â”œâ”€ Activates virtual environment
   â””â”€ Calls main.py in TrafficLearningproject/src/
   
2. main.py
   â”œâ”€ Loads config.py (hyperparameters, paths)
   â”œâ”€ Creates TrafficEnv from traffic_env.py
   â”‚  â””â”€ traffic_env connects to sumo_interface.py
   â”‚     â””â”€ sumo_interface starts SUMO simulator
   â”‚
   â”œâ”€ Phase 1: BASELINE EVALUATION (with progress bars)
   â”‚  â”œâ”€ Tests Fixed-Time Controller (5 runs) â†’ baseline_results.csv
   â”‚  â””â”€ Tests Actuated Controller (5 runs) â†’ baseline_results.csv
   â”‚
   â”œâ”€ Phase 2: MADRL TRAINING (200 episodes)
   â”‚  â”œâ”€ For each episode:
   â”‚  â”‚  â”œâ”€ Reset env (traffic_env.py)
   â”‚  â”‚  â”œâ”€ Collect actions from PPOTrainer (ppo_trainer.py)
   â”‚  â”‚  â”‚  â””â”€ Uses Actor network (actor.py)
   â”‚  â”‚  â”œâ”€ Step environment (traffic_env.py)
   â”‚  â”‚  â”œâ”€ Store experience in Buffer (buffer.py)
   â”‚  â”‚  â”œâ”€ Update networks when buffer full
   â”‚  â”‚  â”‚  â”œâ”€ Actor (policy gradient)
   â”‚  â”‚  â”‚  â””â”€ Critic (value regression)
   â”‚  â”‚  â””â”€ Log metrics to TensorBoard
   â”‚  â”‚
   â”‚  â””â”€ Save models: actor_final.pt, critic_final.pt
   â”‚
   â”œâ”€ Phase 3: BEFORE/AFTER COMPARISON
   â”‚  â””â”€ Displays baseline vs trained agent performance
   â”‚
   â””â”€ Saves all metrics to logs/metrics.json

3. Outputs
   â”œâ”€ Console: Before/after comparison + progress
   â”œâ”€ logs/metrics.json: Complete training history
   â”œâ”€ logs/baseline_results.csv: Baseline metrics
   â”œâ”€ logs/tensorboard/: Event files for visualization
   â””â”€ models/: All checkpoints and final models
```

---

## ğŸ“Š KEY FILE RELATIONSHIPS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      run_agent.py                    â”‚  (Entry point)
â”‚      (Execution wrapper)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      main.py                         â”‚  (Orchestration)
â”‚      (Training loop)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Imports:                            â”‚
â”‚  â”œâ”€ config.py (hyperparameters)     â”‚
â”‚  â”œâ”€ traffic_env.py (environment)    â”‚
â”‚  â””â”€ ppo_trainer.py (algorithm)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ traffic_env  â”‚  â”‚ ppo_trainer.py   â”‚  â”‚   config.py     â”‚
â”‚ (Env)        â”‚  â”‚ (Algorithm)      â”‚  â”‚ (Settings)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                 â”‚
â”‚ Imports:     â”‚  â”‚ Imports:         â”‚  â”‚ Defines:        â”‚
â”‚â”œsumo_inter   â”‚  â”‚â”œactor.py (net)   â”‚  â”‚ â”œEpisodes       â”‚
â”‚â”œconfig.py    â”‚  â”‚â”œcritic.py (net)  â”‚  â”‚ â”œLearning rates â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â”œbuffer.py (mem)  â”‚  â”‚ â””File paths    â”‚
      â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  sumo_interface.py   â”‚
â”‚  (SUMO control)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Manages:            â”‚
â”‚  â”œ SUMO process      â”‚
â”‚  â”œ TraCI connection  â”‚
â”‚  â”œ Actions          â”‚
â”‚  â”” Observations     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SUMO Simulator     â”‚
â”‚   (4x4 Grid)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ WHAT EACH COMPONENT DOES IN TRAINING

### **Actor Network (actor.py)**
- **Input**: Current traffic state (12 dimensions)
- **Output**: Probability of 2 actions (stay phase, switch phase)
- **Purpose**: Learns optimal traffic light control policy
- **Training**: Updated via policy gradient (PPO loss)

### **Critic Network (critic.py)**
- **Input**: Current traffic state (12 dimensions)
- **Output**: State value + Cost value estimates
- **Purpose**: Estimates how good a state is
- **Training**: Updated via MSE regression loss

### **Buffer (buffer.py)**
- **Stores**: States, actions, rewards, costs, advantages
- **Purpose**: Provides batch data for training
- **Size**: Configurable (default 2048 transitions)

### **PPO Trainer (ppo_trainer.py)**
- **Collects**: Transitions from environment
- **Computes**: Advantages, returns, cost estimates
- **Updates**: Actor (clipped policy loss) and Critic (value loss)
- **Enforces**: Safety constraints via Lagrange multiplier

### **Traffic Environment (traffic_env.py)**
- **Interface**: Between trainer and SUMO simulator
- **Provides**: States, rewards, costs, termination signals
- **Manages**: Episode reset, scenario selection

### **SUMO Interface (sumo_interface.py)**
- **Low-level control**: Direct SUMO process interaction
- **Actions**: Traffic light phase changes
- **Observations**: Vehicle positions, speeds, stops
- **Metrics**: Reward (speeds), Cost (stops)

---

## ğŸ’¾ DATA FLOW DURING TRAINING

```
Episode Start
    â”‚
    â”œâ”€ reset() â†’ Initial state from SUMO
    â”‚
    â”œâ”€ PPOTrainer.step_collect()
    â”‚  â””â”€ Actor network predicts action
    â”‚
    â”œâ”€ env.step(action)
    â”‚  â””â”€ SUMO simulator executes action
    â”‚     â””â”€ Returns: next_state, reward, cost, done
    â”‚
    â”œâ”€ Buffer.store()
    â”‚  â””â”€ Saves transition
    â”‚
    â”œâ”€ If buffer full:
    â”‚  â”œâ”€ Compute advantages & returns
    â”‚  â”œâ”€ Actor loss = -logprob * advantage
    â”‚  â”œâ”€ Critic loss = (value - return)Â²
    â”‚  â”œâ”€ Actor.backward() and optimize
    â”‚  â”œâ”€ Critic.backward() and optimize
    â”‚  â”œâ”€ Update Lagrange multiplier
    â”‚  â””â”€ Clear buffer
    â”‚
    â””â”€ Repeat until episode done (3600 steps)

Episode End
    â”‚
    â”œâ”€ Save metrics to metrics.json
    â”œâ”€ Log to TensorBoard
    â”œâ”€ Check if best episode â†’ save models
    â””â”€ Continue to next episode
```

---

## ğŸ”‘ KEY CONFIGURATION VALUES (config.py)

| Parameter | Value | Purpose |
|-----------|-------|---------|
| `NUM_AGENTS` | 16 | Number of traffic light controllers |
| `STATE_DIM` | 12 | State vector size per agent |
| `ACTION_DIM` | 2 | Available actions (stay/switch) |
| `TOTAL_EPISODES` | 200 | Training episodes |
| `MAX_STEPS_PER_EPISODE` | 3600 | Simulation seconds per episode |
| `LEARNING_RATE_ACTOR` | 3e-4 | Actor network learning rate |
| `LEARNING_RATE_CRITIC` | 1e-3 | Critic network learning rate |
| `BUFFER_SIZE` | 2048 | Transitions before training update |
| `COST_LIMIT` | 60.0 | Safety constraint threshold |
| `LAGRANGE_LR` | 1e-2 | Constraint penalty adaptation rate |

---

## ğŸ“ˆ OUTPUT FILES EXPLANATION

### **metrics.json**
```json
{
  "episodes": [
    {
      "episode": 1,
      "scenario": "medium",
      "reward": 3543.83,      // Total episode reward
      "avg_cost": 0.0563,     // Average cost (0=perfect, 1=all stopped)
      "steps": 3600
    },
    ...
  ],
  "best_reward": 3721.13,     // Highest reward across all episodes
  "best_episode": 34          // Episode number with best reward
}
```

### **baseline_results.csv**
```
Method,Avg Reward,Avg Cost
Fixed-Time,86.23,0.1003
Actuated,84.22,0.1090
```

### **TensorBoard Events**
- Episode/Total_Reward: Reward per episode
- Episode/Average_Cost_Episode: Cost per episode
- Loss/Policy: Actor loss over training
- Loss/Critic: Critic loss over training
- Safety/Lagrange_Multiplier: Constraint penalty value

---

## ğŸš€ QUICK REFERENCE: WHO CALLS WHOM

```
run_agent.py
  â””â”€ main.py
      â”œâ”€ config.py (load settings)
      â”œâ”€ traffic_env.py (create environment)
      â”‚   â””â”€ sumo_interface.py (SUMO control)
      â”œâ”€ ppo_trainer.py (create trainer)
      â”‚   â”œâ”€ actor.py (create policy network)
      â”‚   â”œâ”€ critic.py (create value networks)
      â”‚   â””â”€ buffer.py (create replay buffer)
      â”œâ”€ evaluate_baseline_fixed_time()
      â”œâ”€ evaluate_baseline_actuated()
      â”œâ”€ Training loop (200 episodes)
      â”‚   â”œâ”€ env.reset()
      â”‚   â”œâ”€ trainer.step_collect()
      â”‚   â”œâ”€ env.step(action)
      â”‚   â”œâ”€ trainer.store()
      â”‚   â”œâ”€ trainer.train_step()
      â”‚   â””â”€ TensorBoard logging
      â””â”€ _print_before_after_comparison()
```

---

## âœ… FILE SUMMARY TABLE

| File | Type | Lines | Purpose |
|------|------|-------|---------|
| main.py | Core | ~400 | Training orchestration |
| config.py | Config | ~100 | Hyperparameters & paths |
| traffic_env.py | Env | ~150 | RL environment wrapper |
| sumo_interface.py | Env | ~200 | SUMO simulator control |
| ppo_trainer.py | Algorithm | ~300 | PPO training logic |
| buffer.py | Memory | ~150 | Experience replay buffer |
| actor.py | Neural Net | ~80 | Policy network |
| critic.py | Neural Net | ~100 | Value networks |
| run_agent.py | Launcher | ~100 | Execution wrapper |
| test_main_quick.py | Test | ~50 | Quick validation |
| test_quick.py | Test | ~40 | SUMO test |
| reset_environment.sh | Script | ~50 | Reset & cleanup |

---

## ğŸ“ LEARNING PATH

To understand the code:

1. **Start here**: `main.py` - See overall training flow
2. **Then read**: `config.py` - Understand all settings
3. **Environment**: `traffic_env.py` â†’ `sumo_interface.py`
4. **Algorithm**: `ppo_trainer.py` â†’ `buffer.py`
5. **Networks**: `actor.py` and `critic.py`

---

**Status**: âœ… This is a complete, production-ready MADRL traffic control system!
